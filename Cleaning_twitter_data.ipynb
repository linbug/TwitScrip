{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is a project that I did to get used to using the Twitter API to scrape tweets, and then cleaning them up and analysing them using Python.** \n",
    "\n",
    "Twitter has two kinds of API: [REST APIs](https://dev.twitter.com/rest/public) and [Streaming APIs](https://dev.twitter.com/streaming/overview) (they also have [ADs APIs](https://dev.twitter.com/ads/overview) but I'm ignoring them):\n",
    "- The REST APIs are used for programmatically reading and writing to Twitter e.g. you might use them to write a new tweet, or read follower data. \n",
    "- The Streaming APIs give you 'low latency' (read - essentially real-time) access to the live stream of tweets. Using the Streaming APIs feels like siphoning off water from a firehose - you set up a streaming connection with Twitter that will *keep giving you fresh tweets until you shut it off*. The Streaming APIs are much better suited to data mining, as you can use them to grab a high volume of tweets.\n",
    "\n",
    "I used the Streaming API to pull public tweets, filtering according to various keywords. There are lots of Python wrappers for the twitter APIs; I used [tweepy](http://www.tweepy.org/). \n",
    "\n",
    "Using the Twitter API first requires you to register an application with them, so that you can get the Consumer Key, Consumer Secret, and Access Tokens required for [OAuth](https://dev.twitter.com/oauth/overview/faq) authentication (a prereq for accessing the API). \n",
    "\n",
    "The code for setting up the connection to Twitter and filtering the stream is in a separate python script. The tweets are delivered back to you in multiple JSON files. An important thing to note is that Twitter will only give you a 1% 'statistically relevant' sample of the total tweets; if you want to grab 100% the tweets for a given filter, you need [special permission](https://dev.twitter.com/streaming/reference/get/statuses/firehose). This notebook shows the cleaning and analysing steps that came after the scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import dependencies\n",
    "import pandas as pd\n",
    "import json\n",
    "import collections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to open up txt file, then read each json file line by line into a Python list\n",
    "def open_and_load_tweets(file_name):\n",
    "    data = []\n",
    "    data_file = open(file_name, 'r')\n",
    "    for line in data_file:\n",
    "        try:\n",
    "            tweet = json.loads(line)\n",
    "            data.append(tweet)\n",
    "        except:\n",
    "            continue\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "thanksgiving = open_and_load_tweets('thanksgiving2.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to read in text from each tweet, first checking whether the field exists\n",
    "def has_text(tweet):\n",
    "    try:\n",
    "        tweet['text']\n",
    "    except:\n",
    "        return False\n",
    "    if tweet['text'] is not None: \n",
    "        return tweet['text']\n",
    "    else:\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets_df = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "thanksgiving_df = pd.DataFrame()\n",
    "thanksgiving_df['text'] = [has_text(tweet) for tweet in thanksgiving]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "983"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(thanksgiving_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No place like home. #Thanksgiving #Family #Fai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A 1904 Vegetarian Thanksgiving Dinner https://...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'm proud to that I've quite a bit of the Than...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.@bonappetit We didn't think anyone else was a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OK let's see them! Post pictures of that big t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  No place like home. #Thanksgiving #Family #Fai...\n",
       "1  A 1904 Vegetarian Thanksgiving Dinner https://...\n",
       "2  I'm proud to that I've quite a bit of the Than...\n",
       "3  .@bonappetit We didn't think anyone else was a...\n",
       "4  OK let's see them! Post pictures of that big t..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thanksgiving_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are various functions I wrote for parsing in particular fields and dealing with errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ignore_keyerrors(tweet):\n",
    "    try:\n",
    "        tweet['user']['location']\n",
    "    except:\n",
    "        return None\n",
    "    return tweet['user']['location']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def has_geo(tweet):\n",
    "    try:\n",
    "        tweet['geo']\n",
    "    except:\n",
    "        return False\n",
    "    if tweet['geo'] is not None: \n",
    "        return tweet['geo']\n",
    "    else:\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num = 0\n",
    "for tweet in tweets_data:\n",
    "        try:\n",
    "            tweet['user']['location']\n",
    "        except KeyError:\n",
    "            num += 1\n",
    "print(num)\n",
    "            \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def has_coordinates(tweet):\n",
    "    try:\n",
    "        tweet['coordinates']\n",
    "    except:\n",
    "        return False\n",
    "    if tweet['coordinates'] is not None: \n",
    "        return tweet['coordinates']\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do next: automate scraping at certain times of day. E.g. on the hour, for 5 minutes. See how the tweets change at different times of day"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
